{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11e78753",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-27T09:20:46.395983Z",
     "start_time": "2026-01-27T09:20:43.596771Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from scipy import stats\n",
    "from Bio.Seq import Seq\n",
    "from plotnine import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6321f1",
   "metadata": {},
   "source": [
    "## Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3dc66dc0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-27T09:20:47.439004Z",
     "start_time": "2026-01-27T09:20:47.422623Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('../Demo_data/ISM_dict.json', 'rb') as fp:\n",
    "     sequence_dict = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c31b5b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-27T09:20:48.509067Z",
     "start_time": "2026-01-27T09:20:48.500389Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the mutation site:\n",
      "#################################################################\n",
      "8701bp downstream of the TSS of the FILIP1L gene (chr3:100105800)\n",
      "Ref: T\n",
      "Alt: C\n"
     ]
    }
   ],
   "source": [
    "print('View the mutation site:')\n",
    "print('#################################################################')\n",
    "print('8701bp downstream of the TSS of the FILIP1L gene (chr3:100105800)')\n",
    "ref_base = sequence_dict['FILIP1L_ref_seq'][147456//2+8701]\n",
    "alt_base = sequence_dict['FILIP1L_alt_seq'][147456//2+8701]\n",
    "print('Ref: '+ref_base)\n",
    "print('Alt: '+alt_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbe3d382",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-27T09:20:50.042599Z",
     "start_time": "2026-01-27T09:20:50.036599Z"
    }
   },
   "outputs": [],
   "source": [
    "FILIP1L_ref_seq = sequence_dict['FILIP1L_ref_seq']\n",
    "FILIP1L_alt_seq = sequence_dict['FILIP1L_alt_seq']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a87356a",
   "metadata": {},
   "source": [
    "## Model load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab65f39c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-27T09:20:50.953019Z",
     "start_time": "2026-01-27T09:20:50.924781Z"
    }
   },
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return torch.nn.functional.gelu(x)\n",
    "\n",
    "class Residual(nn.Module):\n",
    "    def __init__(self, module):\n",
    "        super(Residual, self).__init__()\n",
    "        self.module = module\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.module(x)\n",
    "    \n",
    "class SoftmaxPooling1D(nn.Module):\n",
    "    def __init__(self, pool_size=2, per_channel=False, w_init_scale=0.0):\n",
    "        super(SoftmaxPooling1D, self).__init__()\n",
    "        self.pool_size = pool_size\n",
    "        self.per_channel = per_channel\n",
    "        self.w_init_scale = w_init_scale\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        batch_size, channels, length = inputs.size()\n",
    "        inputs = inputs.view(batch_size, channels, length // self.pool_size, self.pool_size)\n",
    "        weights = torch.softmax(self.w_init_scale * inputs, dim=-1)\n",
    "        return torch.sum(inputs * weights, dim=-1)\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, pool_size):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv = nn.Conv1d(in_channels, out_channels, kernel_size, padding='same')\n",
    "        self.bn = nn.BatchNorm1d(out_channels)\n",
    "        self.pool = nn.MaxPool1d(pool_size)\n",
    "        self.gelu = GELU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.pool(x)\n",
    "        return x\n",
    "    \n",
    "class TargetLengthCrop1D(nn.Module):\n",
    "    def __init__(self, target_length):\n",
    "        super(TargetLengthCrop1D, self).__init__()\n",
    "        self.target_length = target_length\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        if self.target_length is None:\n",
    "            return inputs\n",
    "        trim = (inputs.shape[-1] - self.target_length) // 2\n",
    "        if trim < 0:\n",
    "            return inputs\n",
    "        elif trim == 0:\n",
    "            return inputs\n",
    "        else:\n",
    "            return inputs[..., trim:-trim]\n",
    "\n",
    "class DeepEvo_Seq2Epi(nn.Module):\n",
    "    def __init__(self, channels=1408, num_transformer_layers=6, num_heads=8, pooling_type='attention'):\n",
    "        super(DeepEvo_Seq2Epi, self).__init__()\n",
    "        assert channels % num_heads == 0, \"channels needs to be divisible by num_heads\"\n",
    "\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv1d(4, 256, kernel_size=15, padding='same'),\n",
    "            Residual(ConvBlock(256, 256, kernel_size=1, pool_size=1)),\n",
    "            SoftmaxPooling1D(pool_size=2) if pooling_type == 'attention' else nn.MaxPool1d(pool_size=2)\n",
    "        )\n",
    "\n",
    "        filter_list = self.exponential_linspace_int(256, 512, num=7, divisible_by=32)\n",
    "        conv_blocks = []\n",
    "        for in_channels, out_channels in zip(filter_list[:-1], filter_list[1:]):\n",
    "            conv_blocks.append(ConvBlock(in_channels, out_channels, kernel_size=20, pool_size=1))\n",
    "            conv_blocks.append(Residual(ConvBlock(out_channels, out_channels, kernel_size=1, pool_size=1)))\n",
    "            if pooling_type == 'attention':\n",
    "                conv_blocks.append(SoftmaxPooling1D(pool_size=2))\n",
    "            else:\n",
    "                conv_blocks.append(nn.MaxPool1d(pool_size=2))\n",
    "        self.conv_tower = nn.Sequential(*conv_blocks)\n",
    "\n",
    "        self.crop_final = TargetLengthCrop1D(1152)\n",
    "        self.final_pointwise = nn.Sequential(\n",
    "            ConvBlock(512, 512, kernel_size=1, pool_size=1),\n",
    "            nn.Dropout(0.3),\n",
    "            GELU()\n",
    "        )\n",
    "\n",
    "        self.heads = nn.ModuleDict({\n",
    "            'all_species': \n",
    "            nn.Sequential(\n",
    "                nn.Conv1d(512, 5, kernel_size=1),\n",
    "                nn.Softplus()\n",
    "            )\n",
    "        })\n",
    "    \n",
    "    def forward(self, x1):\n",
    "        x1 = self.stem(x1)\n",
    "        x1 = self.conv_tower(x1)\n",
    "\n",
    "        x1 = self.crop_final(x1)\n",
    "        x1 = self.final_pointwise(x1)\n",
    "        output = self.heads['all_species'](x1)\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def exponential_linspace_int(start, end, num, divisible_by=1):\n",
    "        def _round(x):\n",
    "            return int(np.round(x / divisible_by) * divisible_by)\n",
    "        base = np.exp(np.log(end / start) / (num - 1))\n",
    "        return [_round(start * base**i) for i in range(num)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13c93f7c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-27T09:20:51.601510Z",
     "start_time": "2026-01-27T09:20:51.356586Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_115468/480556414.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n"
     ]
    }
   ],
   "source": [
    "save_path = '../Model/DeepEvo_Seq2Epi_LCL.pth'\n",
    "model_seq2epi = DeepEvo_Seq2Epi()\n",
    "state_dict = torch.load(save_path, map_location='cpu')\n",
    "model_seq2epi.load_state_dict(state_dict, strict=True)\n",
    "model_seq2epi = model_seq2epi.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb7204d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-27T09:20:51.741184Z",
     "start_time": "2026-01-27T09:20:51.724309Z"
    }
   },
   "outputs": [],
   "source": [
    "class DeepEvo_Epi2DEG(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeepEvo_Epi2DEG, self).__init__()\n",
    "        \n",
    "        self.conv1d_shared1 = nn.Conv1d(5, 32, kernel_size=1, stride=1)\n",
    "        self.conv1d_shared2 = nn.Conv1d(32, 64, kernel_size=1, stride=1)\n",
    "        self.conv1d_shared3 = nn.Conv1d(64, 64, kernel_size=1, stride=1)\n",
    "        self.pool1 = nn.AvgPool1d(kernel_size=4, stride=4)\n",
    "        self.pool2 = nn.AvgPool1d(kernel_size=8, stride=8)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.fc1 = nn.Linear(64 * 36, 32)\n",
    "        self.fc2 = nn.Linear(64, 3)\n",
    "\n",
    "    def forward_once(self, x):\n",
    "        x = self.conv1d_shared1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv1d_shared2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.conv1d_shared3(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return x\n",
    "    \n",
    "    def forward(self, input1, input2):\n",
    "        output1 = self.forward_once(input1)\n",
    "        output2 = self.forward_once(input2)\n",
    "        output = torch.cat((output1, output2), dim=1)\n",
    "        output = self.fc2(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4fced19",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-27T09:20:52.215886Z",
     "start_time": "2026-01-27T09:20:52.189188Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_115468/4271429628.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n"
     ]
    }
   ],
   "source": [
    "save_path = '../Model/DeepEvo_Epi2DEG_LCL.pth'\n",
    "model_epi2deg = DeepEvo_Epi2DEG()\n",
    "state_dict = torch.load(save_path, map_location='cpu')\n",
    "model_epi2deg.load_state_dict(state_dict, strict=True)\n",
    "model_epi2deg = model_epi2deg.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4f17c8",
   "metadata": {},
   "source": [
    "## prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4a83099",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-27T09:20:53.005634Z",
     "start_time": "2026-01-27T09:20:52.995784Z"
    }
   },
   "outputs": [],
   "source": [
    "def fill_with_N(s, target_length):\n",
    "    if len(s) <= target_length:\n",
    "        return s.center(target_length, 'N')\n",
    "    else:\n",
    "        current_length = len(s)\n",
    "        trimming_length = current_length - target_length\n",
    "        left_trim = trimming_length // 2\n",
    "        right_trim = trimming_length - left_trim\n",
    "        return s[left_trim:current_length - right_trim]\n",
    "\n",
    "def pad_to_multiple_of_128(s, pad_char='N'):\n",
    "    length = len(s)\n",
    "    padding_needed = (128 - length % 128) % 128\n",
    "    if padding_needed == 0:\n",
    "        return s\n",
    "    left_padding = padding_needed // 2\n",
    "    right_padding = padding_needed - left_padding\n",
    "    padded_string = s + pad_char * padding_needed\n",
    "    return padded_string\n",
    "\n",
    "def one_hot_encode_sequence(sequence):\n",
    "    sequence = sequence.upper()\n",
    "    bases = 'ACGT'\n",
    "    one_hot = np.zeros((len(sequence), 4), dtype=np.int8)\n",
    "    for i, base in enumerate(sequence):\n",
    "        if base in bases:\n",
    "            one_hot[i, bases.index(base)] = 1\n",
    "        else:\n",
    "            one_hot[i, :] = [0, 0, 0, 0]\n",
    "    return one_hot\n",
    "\n",
    "def process_seq(aligned_seq_human, target_length):\n",
    "    aligned_seq_human_no_dash = aligned_seq_human.replace(\"-\", \"\")\n",
    "    aligned_seq_human_no_dash_padding = fill_with_N(aligned_seq_human_no_dash, target_length=target_length)\n",
    "    aligned_seq_human_onehot = one_hot_encode_sequence(aligned_seq_human_no_dash_padding)\n",
    "    return aligned_seq_human_onehot\n",
    "\n",
    "def predict_Epi(module, aligned_seq_human, target_length):\n",
    "    aligned_seq_human_onehot = process_seq(aligned_seq_human, target_length)\n",
    "    aligned_seq_human_onehot = np.expand_dims(aligned_seq_human_onehot, axis=0)\n",
    "    aligned_seq_human_onehot = torch.tensor(aligned_seq_human_onehot)\n",
    "    aligned_seq_human_onehot = torch.transpose(aligned_seq_human_onehot, 1, 2)\n",
    "    aligned_seq_human_onehot = aligned_seq_human_onehot.float()\n",
    "    aligned_seq_human_onehot = aligned_seq_human_onehot\n",
    "    with torch.no_grad():\n",
    "        output_row = module(aligned_seq_human_onehot)\n",
    "    return output_row.to('cpu')\n",
    "\n",
    "def predict_DEG(sequence1, sequence2):\n",
    "    with torch.no_grad():\n",
    "        Epi_res1 = predict_Epi(model_seq2epi, sequence1, 180224)\n",
    "        Epi_res2 = predict_Epi(model_seq2epi, sequence2, 180224)\n",
    "        output = model_epi2deg(Epi_res1, Epi_res2)\n",
    "    output_cpu = output.to('cpu')\n",
    "    non_deg_score = output_cpu[0, 0].item()\n",
    "    seq1_high_score = output_cpu[0, 1].item()\n",
    "    seq2_high_score = output_cpu[0, 2].item()\n",
    "    scores = {\n",
    "        'NonDEG': non_deg_score,\n",
    "        'Seq1_High': seq1_high_score,\n",
    "        'Seq2_High': seq2_high_score\n",
    "    }\n",
    "    \n",
    "    return scores\n",
    "\n",
    "def in_silico_mutation(sequence1, sequence2):\n",
    "    scores_1 = predict_DEG(sequence1, sequence2)\n",
    "    scores_2 = predict_DEG(sequence2, sequence1)\n",
    "    EIS = scores_1['Seq2_High'] - scores_2['Seq2_High'] \n",
    "    return EIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b2df3a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-27T09:21:01.547062Z",
     "start_time": "2026-01-27T09:20:53.755371Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/BGM/qij/miniconda3/envs/deep_learning/lib/python3.10/site-packages/torch/nn/modules/conv.py:304: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at ../aten/src/ATen/native/Convolution.cpp:1031.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.100259680300951\n"
     ]
    }
   ],
   "source": [
    "FILIP1L_ref_seq = sequence_dict['FILIP1L_ref_seq']\n",
    "FILIP1L_alt_seq = sequence_dict['FILIP1L_alt_seq']\n",
    "EIS = in_silico_mutation(FILIP1L_ref_seq, FILIP1L_alt_seq)\n",
    "print(EIS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96b011f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "deep_learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
